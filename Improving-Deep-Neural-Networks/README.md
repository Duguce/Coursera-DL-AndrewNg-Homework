# 1. Improving-Deep-Neural-Networks
ä½œä¸šå®ç°ï¼šæ”¹å–„æ·±å±‚ç¥ç»ç½‘ç»œéƒ¨åˆ†

| åºå· | é¡¹ç›®åç§° |                           å¤‡æ³¨                            |
| :--: | :------: | :-------------------------------------------------------: |
|  1   |  Week1   | æ·±åº¦å­¦ä¹ çš„å®è·µå±‚é¢ (Practical aspects of Deep  Learning)  |
|  2   |  Week2   |            ä¼˜åŒ–ç®—æ³• (Optimization algorithms)             |
|  3   |  Week3   | è¶…å‚æ•°è°ƒè¯•ã€Batchæ­£åˆ™åŒ–å’Œç¨‹åºæ¡†æ¶ (Hyperparameter tuning) |





------

<!-- TOC -->

- [1. Improving-Deep-Neural-Networks](#1-improving-deep-neural-networks)
  - [1.1. æ·±åº¦å­¦ä¹ çš„å®è·µå±‚é¢](#11-æ·±åº¦å­¦ä¹ çš„å®è·µå±‚é¢)
    - [1.1.1. è®­ç»ƒï¼ŒéªŒè¯ï¼Œæµ‹è¯•é›†ï¼ˆTrain / Dev / Test setsï¼‰](#111-è®­ç»ƒéªŒè¯æµ‹è¯•é›†train--dev--test-sets)
    - [1.1.2. åå·®ï¼Œæ–¹å·®ï¼ˆBias / Varianceï¼‰](#112-åå·®æ–¹å·®bias--variance)
    - [1.1.3. æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰](#113-æ­£åˆ™åŒ–regularization)
    - [1.1.4. å½’ä¸€åŒ–è¾“å…¥ï¼ˆNormalizing inputsï¼‰](#114-å½’ä¸€åŒ–è¾“å…¥normalizing-inputs)
    - [1.1.5. æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸ï¼ˆVanishing / Exploding gradientsï¼‰](#115-æ¢¯åº¦æ¶ˆå¤±æ¢¯åº¦çˆ†ç‚¸vanishing--exploding-gradients)
    - [1.1.6. ç¥ç»ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–ï¼ˆWeight Initialization for Deep Networksï¼‰](#116-ç¥ç»ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–weight-initialization-for-deep-networks)
  - [1.2. ä¼˜åŒ–ç®—æ³•](#12-ä¼˜åŒ–ç®—æ³•)
    - [1.2.1. Mini-batch æ¢¯åº¦ä¸‹é™ï¼ˆMini-batch gradient descentï¼‰](#121-mini-batch-æ¢¯åº¦ä¸‹é™mini-batch-gradient-descent)
    - [1.2.2. åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient descent with Momentumï¼‰](#122-åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•gradient-descent-with-momentum)
    - [1.2.3.Adam ä¼˜åŒ–ç®—æ³•ï¼ˆAdam optimization algorithmï¼‰](#123adam-ä¼˜åŒ–ç®—æ³•adam-optimization-algorithm)
  - [1.3. è¶… å‚ æ•° è°ƒ è¯• ã€ Batch æ­£ åˆ™ åŒ– å’Œ ç¨‹ åº æ¡† æ¶ï¼ˆHyperparameter tuningï¼‰](#13-è¶…-å‚-æ•°-è°ƒ-è¯•--batch-æ­£-åˆ™-åŒ–-å’Œ-ç¨‹-åº-æ¡†-æ¶hyperparameter-tuning)
    - [1.3.1. è°ƒè¯•å¤„ç†ï¼ˆTuning processï¼‰](#131-è°ƒè¯•å¤„ç†tuning-process)
    - [1.3.2. å°† Batch Norm æ‹Ÿåˆè¿›ç¥ç»ç½‘ç»œï¼ˆFitting Batch Norm into a neural networkï¼‰](#132-å°†-batch-norm-æ‹Ÿåˆè¿›ç¥ç»ç½‘ç»œfitting-batch-norm-into-a-neural-network)
    - [1.3.3. Softmax å›å½’ï¼ˆSoftmax regressionï¼‰](#133-softmax-å›å½’softmax-regression)

<!-- /TOC -->



## 1.1. æ·±åº¦å­¦ä¹ çš„å®è·µå±‚é¢

### 1.1.1. è®­ç»ƒï¼ŒéªŒè¯ï¼Œæµ‹è¯•é›†ï¼ˆTrain / Dev / Test setsï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- å¾ªç¯è¿­ä»£è¿‡ç¨‹çš„æ•ˆç‡æ˜¯å†³å®šä¸€ä¸ªé¡¹ç›®è¿›å±•çš„å…³é”®å› ç´ ï¼›
- æœºå™¨å­¦ä¹ ä¸­ï¼Œé€šå¸¸å°†æ ·æœ¬åˆ†æˆè®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ä¸‰éƒ¨åˆ†ï¼›
- å¯¹äºå°æ•°æ®é›†åˆ†å‰²æ ‡å‡†ï¼š70%éªŒè¯é›†ï¼Œ30%æµ‹è¯•é›†ï¼›
- æ•°æ®é›†è§„æ¨¡è¾ƒå¤§æ—¶ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†è¦å°äºæ•°æ®æ€»é‡çš„20%æˆ–10%ï¼›
- æµ‹è¯•é›†çš„ç›®çš„æ˜¯å¯¹æœ€ç»ˆæ‰€é€‰å®šçš„ç¥ç»ç½‘ç»œç³»ç»Ÿåšå‡ºæ— åä¼°è®¡ï¼›

### 1.1.2. åå·®ï¼Œæ–¹å·®ï¼ˆBias / Varianceï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- é«˜åå·®å¯¹åº”æ¬ æ‹Ÿåˆï¼Œé«˜æ–¹å·®å¯¹åº”è¿‡æ‹Ÿåˆï¼›
- éªŒè¯é›†è¯¯å·®å¤§ ==> é«˜æ–¹å·®ï¼›
- è®­ç»ƒé›†è¯¯å·®å¤§ ==> é«˜åå·®ï¼›

- åˆå§‹æ¨¡å‹è®­ç»ƒå®Œï¼Œå¦‚æœåå·®è¾ƒé«˜ï¼Œå¯ä»¥å°è¯•è¯„ä¼°è®­ç»ƒé›†æˆ–è®­ç»ƒæ•°æ®çš„æ€§èƒ½ï¼Œå†åˆ™ï¼Œå¯ä»¥å°è¯•æ–°çš„ç½‘ç»œæ¶æ„ï¼›
- æ¨¡å‹è®­ç»ƒç›®æ ‡ï¼Œå°½å¯èƒ½æ‰¾åˆ°ä¸€ä¸ªä½åå·®ï¼Œä½æ–¹å·®çš„æ¶æ„ï¼›

### 1.1.3. æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- æ­£åˆ™åŒ–å¯ä»¥è¢«ç”¨äºå‡å°‘**æ–¹å·®**ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼›

- L1æ­£åˆ™åŒ–ï¼Œ æœ€ç»ˆä¼šæ˜¯ç¨€ç–çš„ï¼Œå‘é‡ä¸­æœ‰å¾ˆå¤š0ï¼Œæœ‰åˆ©äºå‹ç¼©æ¨¡å‹ï¼Œå› ä¸ºé›†åˆä¸­å‚æ•°å‡ä¸º0ï¼Œå­˜å‚¨æ¨¡å‹æ‰€å ç”¨çš„å†…å­˜æ›´å°‘ã€‚å®é™…ä¸Šï¼Œè™½ç„¶L1æ­£åˆ™åŒ–ä½¿æ¨¡å‹å˜å¾—ç¨€ç–ï¼Œå´æ²¡æœ‰é™ä½å¤ªå¤šå­˜å‚¨å†…å­˜ï¼›
- ***L2æ­£åˆ™åŒ–***æ˜¯æœ€å¸¸è§çš„æ­£åˆ™åŒ–ç±»å‹ï¼Œä¹Ÿè¢«ç§°ä¸ºâ€œæƒé‡è¡°å‡â€ï¼›
- L2æ­£åˆ™åŒ–é€šè¿‡è®¾ç½®lambdaè°ƒæ•´æƒé‡çŸ©é˜µWçš„å€¼ï¼›

- ***Dropout æ­£åˆ™åŒ–***é€šè¿‡éå†ç½‘ç»œçš„æ¯ä¸€å±‚ï¼Œå¹¶è®¾ç½®æ¶ˆé™¤ç¥ç»ç½‘ç»œä¸­èŠ‚ç‚¹çš„æ¦‚ç‡ï¼ˆDropoutå¯ä»¥éšæœºåˆ é™¤ç½‘ç»œä¸­çš„ç¥ç»ç½‘ç»œï¼‰ï¼Œè®¾ç½®å®ŒèŠ‚ç‚¹æ¦‚ç‡ï¼Œæˆ‘ä»¬ä¼šæ¶ˆé™¤ä¸€äº›èŠ‚ç‚¹ï¼Œç„¶ååˆ é™¤æ‰ä»è¯¥èŠ‚ç‚¹è¿›å‡ºçš„è¿çº¿ï¼Œæœ€åå¾—åˆ°ä¸€ä¸ªèŠ‚ç‚¹æ›´å°‘ï¼Œè§„æ¨¡æ›´å°çš„ç½‘ç»œï¼Œç„¶åç”¨backpropæ–¹æ³•è¿›è¡Œè®­ç»ƒï¼›

- inverted dropout (åå‘éšæœºå¤±æ´»)ï¼›

```python
d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep-prob
a3 = np.multiply(a3, d3)
a3 /= keep-prob
```

- Dropoutçš„ä¸€å¤§ç¼ºç‚¹å°±æ˜¯ä»£ä»·å‡½æ•° ä¸å†è¢«æ˜ç¡®å®šä¹‰ï¼Œæ¯æ¬¡è¿­ä»£ï¼Œéƒ½ä¼šéšæœºç§»é™¤ä¸€äº›èŠ‚ç‚¹ï¼Œ å¦‚æœå†ä¸‰æ£€æŸ¥æ¢¯åº¦ä¸‹é™çš„æ€§èƒ½ï¼Œå®é™…ä¸Šæ˜¯å¾ˆéš¾è¿›è¡Œå¤æŸ¥çš„ï¼›
- å¸¸è§æ­£åˆ™åŒ–æ–¹æ³•ï¼š1ï¼‰L2æ­£åˆ™åŒ– 2ï¼‰éšæœºå¤±æ´»ï¼ˆdropoutï¼‰æ­£åˆ™åŒ– 3ï¼‰æ•°æ®æ‰©å¢ 4ï¼‰early stoppingï¼›

### 1.1.4. å½’ä¸€åŒ–è¾“å…¥ï¼ˆNormalizing inputsï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- å½’ä¸€åŒ–åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šé›¶å‡å€¼å’Œå½’ä¸€åŒ–æ–¹å·®ï¼›
- å½’ä¸€åŒ–å¯ä»¥åŠ é€Ÿç¥ç»ç½‘ç»œçš„è®­ç»ƒï¼ˆç‰¹å¾éƒ½åœ¨ç›¸ä¼¼èŒƒå›´å†…ï¼Œè€Œä¸æ˜¯ä»1åˆ°1000ï¼Œ0åˆ°1çš„èŒƒå›´ï¼Œè€Œæ˜¯åœ¨-1åˆ°1èŒƒå›´å†…æˆ–ç›¸ä¼¼åå·®ï¼Œè¿™ä½¿å¾—ä»£ä»·å‡½æ•°ä¼˜åŒ–èµ·æ¥æ›´ç®€å•å¿«é€Ÿï¼‰ï¼›

### 1.1.5. æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸ï¼ˆVanishing / Exploding gradientsï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- ï¼ˆæ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸ æŒ‡ï¼‰å¯¼æ•°æˆ–å¡åº¦æœ‰æ—¶ä¼šå˜å¾—éå¸¸å¤§ï¼Œæˆ–è€…éå¸¸å°ï¼Œç”šè‡³äºä»¥æŒ‡æ•°æ–¹å¼å˜å°ï¼Œè¿™åŠ å¤§äº†è®­ç»ƒçš„éš¾åº¦ï¼›

### 1.1.6. ç¥ç»ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–ï¼ˆWeight Initialization for Deep Networksï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

![æƒé‡åˆå§‹åŒ–å…¬å¼](https://latex.codecogs.com/svg.image?w^{[l]}=&space;np.random.randn(shape)&space;*&space;np.sqrt(\frac{1}{n^{[l-1]}}))

## 1.2. ä¼˜åŒ–ç®—æ³•

### 1.2.1. Mini-batch æ¢¯åº¦ä¸‹é™ï¼ˆMini-batch gradient descentï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- æŠŠè®­ç»ƒé›†åˆ†å‰²ä¸ºå°ä¸€ç‚¹çš„å­é›†è®­ç»ƒï¼Œè¿™äº›å­é›†è¢«å–åä¸º mini-batchï¼›
- mini-batchæ¢¯åº¦ä¸‹é™æ³•æ¯”batchæ¢¯åº¦ä¸‹é™æ³•è¿è¡Œåœ°æ›´å¿«ï¼›
- éšæœºæ¢¯åº¦ä¸‹é™æ³•çš„ä¸€å¤§ç¼ºç‚¹æ˜¯ï¼Œä½ ä¼šå¤±å»æ‰€æœ‰å‘é‡åŒ–å¸¦ç»™ä½ çš„åŠ é€Ÿï¼Œå› ä¸ºä¸€æ¬¡æ€§åªå¤„ç†äº†ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè¿™æ ·æ•ˆç‡è¿‡äºä½ä¸‹ï¼Œæ‰€ä»¥å®è·µä¸­æœ€å¥½é€‰æ‹©ä¸å¤§ä¸å°çš„ mini-batch å°ºå¯¸ï¼Œå®é™…ä¸Šå­¦ä¹ ç‡è¾¾åˆ°æœ€å¿«ï¼›

### 1.2.2. åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•ï¼ˆGradient descent with Momentumï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•å¯ä»¥åŠ å¿«æ¢¯åº¦ä¸‹é™ï¼Œç®€è€Œè¨€ä¹‹ï¼ŒåŸºæœ¬æ€æƒ³å°±æ˜¯è®¡ç®—æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡æ•°ï¼Œå¹¶åˆ©ç”¨è¯¥æ¢¯åº¦æ›´æ–°æƒé‡ï¼›

### 1.2.3.Adam ä¼˜åŒ–ç®—æ³•ï¼ˆAdam optimization algorithmï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- Adam ä¼˜åŒ–ç®—æ³•æ˜¯å°†Momentumå’ŒRMSpropç®—æ³•ç›¸ç»“åˆï¼›

## 1.3. è¶… å‚ æ•° è°ƒ è¯• ã€ Batch æ­£ åˆ™ åŒ– å’Œ ç¨‹ åº æ¡† æ¶ï¼ˆHyperparameter tuningï¼‰

### 1.3.1. è°ƒè¯•å¤„ç†ï¼ˆTuning processï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- è¶…å‚æ•°è°ƒè¯•å€¼é€‰æ‹©æ–¹æ³•ï¼šéšæœºé€‰ç‚¹æ³•å’Œç”±ç²—ç³™åˆ°ç²¾ç»†çš„ç­–ç•¥ï¼›

### 1.3.2. å°† Batch Norm æ‹Ÿåˆè¿›ç¥ç»ç½‘ç»œï¼ˆFitting Batch Norm into a neural networkï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- Batchæ­£åˆ™åŒ–

### 1.3.3. Softmax å›å½’ï¼ˆSoftmax regressionï¼‰

ğŸŒ± **å…³é”®ç‚¹ï¼š**

- Softmax
